{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- transfer image over network -> if high size cause delay\n",
    "- compress -> transfer\n",
    "- self driving cars -> need to do semantic segmentation to detect different kind of objects \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/auto.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/auto2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- encoder,bottleneck,decoder\n",
    "- output represents reconstructed input hence same dimention\n",
    "- object is to learn a representation that will minimize reconstruction loss\n",
    "- learning can be done by recirculation or back propagation\n",
    "- trivial solution is just copy of image with 0 loss\n",
    "  -  means not learn \n",
    "  - one way to work around is constrain properties of hidden layer, like size and activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/some.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- even if no of neurons in hidden layer is less than input layer,it will learn something"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. activation\n",
    "- no activation and mse as loss gives same result as pca\n",
    "![](images/pca.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- latent representation of k neurons will represent top k principle components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- regularized auto encoders\n",
    "   - same principle of vanilla auto encoders\n",
    "    but have modified loss function \n",
    "![](images/reg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- thinking of sparsity think most neurons are off.\n",
    "- sigmoid activation function -> \n",
    "![](images/regg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\hat{\\rho}_j$ sparcity parameter\n",
    "- ${\\rho}_j$ =0.05\n",
    "- we set ${\\rho}_j$ =0.05 and penalize all neuron with  $\\hat{\\rho}_j$ that deviates from ${\\rho}_j$\n",
    "- PROBABILITY OF SUCCESS\n",
    "  - $P_j$ =$Ber({\\rho}_j)$\n",
    "  - $q_j$ =$Ber(\\hat{\\rho}_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we impose sparsity constraint by making average activation output of each hidden neuron  some very low value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/kl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/1.png)\n",
    "![](images/2.png)\n",
    "![](images/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  The latent representation is the compressed, lower-dimensional encoding that captures the essential features of the input data.\n",
    "#### Key points regarding the latent representation in autoencoders:\n",
    "\n",
    "- Dimensionality Reduction: The latent representation typically has a lower dimensionality compared to the input data. This allows the autoencoder to learn a more compact representation of the data, which can be useful for tasks like noise reduction, anomaly detection, or data compression.\n",
    "\n",
    "- Feature Extraction: The encoder network learns to map the input data to the latent space, ideally capturing the most relevant features of the data. The quality of the latent space representation is crucial, as it determines how well the decoder can reconstruct the input.\n",
    "\n",
    "- Interpretability and Structure: In some applications, understanding or interpreting the latent space is important. For example, in variational autoencoders (VAEs), the latent space is often structured to encourage smoothness (i.e., nearby points in latent space correspond to similar input data), which can facilitate interpolation or generation of new data points.\n",
    "\n",
    "- Compression: The latent space can also be viewed as a form of data compression. The model must learn to represent the input data in a lower-dimensional form while preserving important characteristics for accurate reconstruction.\n",
    "\n",
    "- Anomaly Detection: The latent representation can also be used for anomaly detection. If an input significantly deviates from the typical latent space patterns, it may indicate an anomaly in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
